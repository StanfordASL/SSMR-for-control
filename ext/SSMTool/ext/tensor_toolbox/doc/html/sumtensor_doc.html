
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Sums of structured tensors</title><meta name="generator" content="MATLAB 9.2"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-09-09"><meta name="DC.source" content="sumtensor_doc.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Sums of structured tensors</h1><!--introduction--><p>When certain operations are performed on a tensor which is formed as a sum of tensors, it can be beneficial to avoid explicitly forming the sum. For example, if a tensor is formed as a sum of a low rank tensor and a sparse tensor, the structure of the summands can make storage, decomposition and operations with other tensors significantly more efficient. The tensor toolbox supports a <tt>sumtensor</tt> object designed to exploit this structure. Here we explain the basics of defining and using sumtensors.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Creating sumtensors</a></li><li><a href="#2">An Large-Scale Example</a></li><li><a href="#4">Further examples of the sumtensor constructer</a></li><li><a href="#6">Use ndims and size for the dimensions of a sumtensor</a></li><li><a href="#7">Use full to convert a sumtensor to a "generic" tensor</a></li><li><a href="#8">Use double to convert a sumtensor to a multidimensional array</a></li><li><a href="#9">Matricized Khatri-Rao product of a sumtensor</a></li><li><a href="#10">Use innerprod to compute the inner product of a sumtensor</a></li><li><a href="#11">Use norm for compatibility with the other types of tensors.</a></li><li><a href="#13">Use cp_als to find a CP decomposition of a sumtensor</a></li><li><a href="#16">Basic operations (plus) for sumtensors</a></li><li><a href="#17">Subscripted reference for sumtensors</a></li></ul></div><h2 id="1">Creating sumtensors</h2><p>A sumtensor T can only be delared as a sum of same-sized tensors <tt>T1, T2,...,TN</tt>. The summand tensors are stored in a cell array, which define the "parts" of the sumtensor. The parts of a sumtensor can be (generic) tensors (as <tt>tensor</tt>), sparse tensors (as <tt>sptensor</tt>), Kruskal tensors (as <tt>ktensor</tt>), or Tucker tensors (as <tt>ttensor</tt>). An example of the use of the <tt>sumtensor</tt> constructor follows.</p><pre class="codeinput">T1 = tensor(ones(3,3,3)); <span class="comment">%&lt;--A tensor</span>
T2 = sptensor([1 1 1; 2 2 2; 3 3 2; 2 1 1], 1, [3,3,3]); <span class="comment">%&lt;--A sparse tensor</span>

T = sumtensor(T1,T2)
</pre><pre class="codeoutput">T is a sumtensor of size 3 x 3 x 3 with 2 parts
T.part{1} is a tensor of size 3 x 3 x 3
	T.part{1}(:,:,1) = 
	     1     1     1
	     1     1     1
	     1     1     1
	T.part{1}(:,:,2) = 
	     1     1     1
	     1     1     1
	     1     1     1
	T.part{1}(:,:,3) = 
	     1     1     1
	     1     1     1
	     1     1     1
T.part{2} is a sparse tensor of size 3 x 3 x 3 with 4 nonzeros
	(1,1,1)     1
	(2,1,1)     1
	(2,2,2)     1
	(3,3,2)     1
</pre><h2 id="2">An Large-Scale Example</h2><p>For large-scale problems, the <tt>sumtensor</tt> class may make the difference as to whether or not a tensor can be stored in memory. Consider the following example, where <img src="sumtensor_doc_eq06518435724312786793.png" alt="$\mathcal{T}$" style="width:13px;height:12px;"> is of size <img src="sumtensor_doc_eq16571189024487911857.png" alt="$1000 x 1000 x 1000$" style="width:104px;height:10px;">, formed from the sum of a <tt>ktensor</tt> and an <tt>sptensor</tt>.</p><pre class="codeinput">X1 = rand(500, 3); <span class="comment">%Generating some factor matrices</span>
X2 = rand(500, 3);
X3 = rand(500, 3);
K = ktensor([1; 1; 1], X1, X2, X3);
S = sptenrand([500, 500, 500], 1e-100);

ST = sumtensor(K,S); <span class="comment">%&lt;-- Declare the sumtensor</span>
TT = full(ST); <span class="comment">%&lt;-- Form the sum of the tensors explicitly</span>

whos <span class="string">ST</span> <span class="string">TT</span> <span class="comment">%&lt;--Output the storage information for these variables</span>
</pre><pre class="codeoutput">  Name        Size                      Bytes  Class        Attributes

  ST        500x500x500                 37696  sumtensor              
  TT        500x500x500            1000000376  tensor                 

</pre><p>The difference in memory between the full and sumtensor is a factor of 10^5! Hence we prefer to use the sumtensor object whenever possible.</p><h2 id="4">Further examples of the sumtensor constructer</h2><p>The sumtensor constructor can declare an empty sumtensor object, having no parts, as follows</p><pre class="codeinput">P = sumtensor()
</pre><pre class="codeoutput">P is an empty sumtensor
</pre><p><tt>sumtensor</tt> also supports use as a copy constructor.</p><pre class="codeinput">S = sumtensor(P)
</pre><pre class="codeoutput">S is an empty sumtensor
</pre><h2 id="6">Use ndims and size for the dimensions of a sumtensor</h2><p>For a given sumtensor, <tt>ndims</tt> returns the number of modes of a sumtensor. Similarly, <tt>size</tt> returns a size array of the sumtensor.</p><pre class="codeinput">ndims(T)
size(T)
</pre><pre class="codeoutput">
ans =

     3


ans =

     3     3     3

</pre><h2 id="7">Use full to convert a sumtensor to a "generic" tensor</h2><p>The <tt>full</tt> function can be used to convert a sumtensor to a generic tensor. Note that for large-scale tensors, this can a large amount of memory because each part of the sumtensor will be expanded and then summed.</p><pre class="codeinput">full(T)
</pre><pre class="codeoutput">ans is a tensor of size 3 x 3 x 3
	ans(:,:,1) = 
	     2     1     1
	     2     1     1
	     1     1     1
	ans(:,:,2) = 
	     1     1     1
	     1     2     1
	     1     1     2
	ans(:,:,3) = 
	     1     1     1
	     1     1     1
	     1     1     1
</pre><h2 id="8">Use double to convert a sumtensor to a multidimensional array</h2><p>The <tt>double</tt> function can be used to convert a sumtensor to a multidimensional array. Similarly to the <tt>full</tt> expansion, this can use a prohibitive amount of memory for large-scale problems.</p><pre class="codeinput">double(T)
</pre><pre class="codeoutput">
ans(:,:,1) =

     2     1     1
     2     1     1
     1     1     1


ans(:,:,2) =

     1     1     1
     1     2     1
     1     1     2


ans(:,:,3) =

     1     1     1
     1     1     1
     1     1     1

</pre><h2 id="9">Matricized Khatri-Rao product of a sumtensor</h2><p>The <tt>mttkrp</tt> function computes the Khatri-Rao product of a matricized tensor and a sumtensor. The required arguments are: a sumtensor X, a cell array of matrices U={U1,...,Um}, and a mode n. The cell array must consist of m matrices, where m is the number of modes in X. The number of columns of these matrices should be constant, and number of rows of matrix Ui should match the dimension of the tensor X in mode i. The matricized Khatri-Rao product operation on sumtensor distributes the operation to the summands of the sumtensor. For details of this specific computation, see the mttkrp documentation for a generic tensor. An example of the use of <tt>mttkrp</tt> follows.</p><pre class="codeinput">U={eye(3), ones(3,3), randn(3,3)}; <span class="comment">%&lt;--The cell array of matrices</span>
mttkrp(T,U,2)
</pre><pre class="codeoutput">
ans =

   -1.8319   -2.6403    1.1656
   -0.7620   -0.2925    1.1656
   -0.7620   -1.3297    1.2842

</pre><h2 id="10">Use innerprod to compute the inner product of a sumtensor</h2><p>The <tt>innerprod</tt> function computes the inner product of a sumtensor T and any type of tensor. The operation is performed by distributing across each of the sumtensor's parts.</p><pre class="codeinput">S = sptensor([1 1 1; 2 1 3; 3 2 2; 2 1 1], 1, [3,3,3]);
innerprod(T,S)
</pre><pre class="codeoutput">
ans =

     6

</pre><h2 id="11">Use norm for compatibility with the other types of tensors.</h2><p>The <tt>norm</tt> function returns 0 and a warning when called on a sumtensor. The procedure of computing the Frobenius norm of a sumtensor does not distribute across its parts, and hence is not supported for sumtensors. This default behavior is provided in order to ensure compatibility of the sumtensor class with existing decomposition routines.</p><pre class="codeinput">norm(T)
</pre><pre class="codeoutput">Warning: The NORM function is not supported by SUMTENSOR. Returning zero. 

ans =

     0

</pre><p>In order avoid this default behavior and return the Frobenius norm of a sumtensor, it can be converted to a tensor using <tt>full</tt>.</p><pre class="codeinput">norm(full(T))
</pre><pre class="codeoutput">
ans =

    6.2450

</pre><h2 id="13">Use cp_als to find a CP decomposition of a sumtensor</h2><p>One of the primary motivations for defining the <tt>sumtensor</tt> class is for efficient decomposition. In particular, when trying to find a CP decomposition of a tensor using alternating least squares, the subproblems can be efficiently created and solved using <tt>mttkrp</tt> and <tt>innerprod</tt>. Both of these operations can be performed more efficiently by exploiting extra structure in the tensors which form the sum, so the performance of <tt>cp_als</tt> is also improved. Consider the following example, where a <tt>cp_als</tt> is run on a sumtensor.</p><pre class="codeinput">cp_als(T, 2)
</pre><pre class="codeoutput">Warning: The NORM function is not supported by SUMTENSOR. Returning zero. 

CP_ALS:
 Iter  1: f = -3.742762e+01 f-delta = 3.7e+01
 Iter  2: f = -3.774831e+01 f-delta = 3.2e-01
 Iter  3: f = -3.778260e+01 f-delta = 3.4e-02
 Iter  4: f = -3.779716e+01 f-delta = 1.5e-02
 Iter  5: f = -3.780460e+01 f-delta = 7.4e-03
 Iter  6: f = -3.780837e+01 f-delta = 3.8e-03
 Iter  7: f = -3.781066e+01 f-delta = 2.3e-03
 Iter  8: f = -3.781237e+01 f-delta = 1.7e-03
 Iter  9: f = -3.781382e+01 f-delta = 1.5e-03
 Iter 10: f = -3.781512e+01 f-delta = 1.3e-03
 Iter 11: f = -3.781631e+01 f-delta = 1.2e-03
 Iter 12: f = -3.781740e+01 f-delta = 1.1e-03
 Iter 13: f = -3.781841e+01 f-delta = 1.0e-03
 Iter 14: f = -3.781933e+01 f-delta = 9.3e-04
 Iter 15: f = -3.782019e+01 f-delta = 8.5e-04
 Iter 16: f = -3.782097e+01 f-delta = 7.9e-04
 Iter 17: f = -3.782170e+01 f-delta = 7.2e-04
 Iter 18: f = -3.782236e+01 f-delta = 6.6e-04
 Iter 19: f = -3.782297e+01 f-delta = 6.1e-04
 Iter 20: f = -3.782353e+01 f-delta = 5.6e-04
 Iter 21: f = -3.782405e+01 f-delta = 5.1e-04
 Iter 22: f = -3.782452e+01 f-delta = 4.7e-04
 Iter 23: f = -3.782495e+01 f-delta = 4.3e-04
 Iter 24: f = -3.782535e+01 f-delta = 3.9e-04
 Iter 25: f = -3.782571e+01 f-delta = 3.6e-04
 Iter 26: f = -3.782604e+01 f-delta = 3.3e-04
 Iter 27: f = -3.782634e+01 f-delta = 3.0e-04
 Iter 28: f = -3.782661e+01 f-delta = 2.7e-04
 Iter 29: f = -3.782686e+01 f-delta = 2.5e-04
 Iter 30: f = -3.782709e+01 f-delta = 2.3e-04
 Iter 31: f = -3.782730e+01 f-delta = 2.1e-04
 Iter 32: f = -3.782749e+01 f-delta = 1.9e-04
 Iter 33: f = -3.782766e+01 f-delta = 1.7e-04
 Iter 34: f = -3.782782e+01 f-delta = 1.6e-04
 Iter 35: f = -3.782796e+01 f-delta = 1.4e-04
 Iter 36: f = -3.782809e+01 f-delta = 1.3e-04
 Iter 37: f = -3.782820e+01 f-delta = 1.2e-04
 Iter 38: f = -3.782831e+01 f-delta = 1.1e-04
 Iter 39: f = -3.782841e+01 f-delta = 9.6e-05
 Final f = -3.782841e+01 
ans is a ktensor of size 3 x 3 x 3
	ans.lambda = [ 4.622      2.6197 ]
	ans.U{1} = 
		    0.4651    0.7251
		    0.5891    0.6429
		    0.6608    0.2468
	ans.U{2} = 
		    0.3998    0.9451
		    0.6327    0.2576
		    0.6632    0.2010
	ans.U{3} = 
		    0.4069    0.9333
		    0.7497    0.1635
		    0.5218    0.3196
</pre><p>It follows that in cases where <img src="sumtensor_doc_eq06518435724312786793.png" alt="$\mathcal{T}$" style="width:13px;height:12px;"> is too large for its full expansion to be stored in memory, we may still be able find a CP decomposition by exploiting the sumtensor structure.</p><p>Note that the fit returned by cp_als is not correct for sumtensors, because the norm operation is not supported.</p><h2 id="16">Basic operations (plus) for sumtensors</h2><p>Sumtensors can be added to any other type of tensor. The result is a new sumtensor with the tensor appended to the parts of the original sumtensor. Note that the tensor is always appended, despite the order of the operation.</p><pre class="codeinput">T+S <span class="comment">%&lt;--S is appended to the parts of T</span>
S+T <span class="comment">%&lt;--S is still the last part of T, despite order</span>
</pre><pre class="codeoutput">ans is a sumtensor of size 3 x 3 x 3 with 3 parts
ans.part{1} is a tensor of size 3 x 3 x 3
	ans.part{1}(:,:,1) = 
	     1     1     1
	     1     1     1
	     1     1     1
	ans.part{1}(:,:,2) = 
	     1     1     1
	     1     1     1
	     1     1     1
	ans.part{1}(:,:,3) = 
	     1     1     1
	     1     1     1
	     1     1     1
ans.part{2} is a sparse tensor of size 3 x 3 x 3 with 4 nonzeros
	(1,1,1)     1
	(2,1,1)     1
	(2,2,2)     1
	(3,3,2)     1
ans.part{3} is a sparse tensor of size 3 x 3 x 3 with 4 nonzeros
	(1,1,1)     1
	(2,1,1)     1
	(2,1,3)     1
	(3,2,2)     1
ans is a sumtensor of size 3 x 3 x 3 with 3 parts
ans.part{1} is a tensor of size 3 x 3 x 3
	ans.part{1}(:,:,1) = 
	     1     1     1
	     1     1     1
	     1     1     1
	ans.part{1}(:,:,2) = 
	     1     1     1
	     1     1     1
	     1     1     1
	ans.part{1}(:,:,3) = 
	     1     1     1
	     1     1     1
	     1     1     1
ans.part{2} is a sparse tensor of size 3 x 3 x 3 with 4 nonzeros
	(1,1,1)     1
	(2,1,1)     1
	(2,2,2)     1
	(3,3,2)     1
ans.part{3} is a sparse tensor of size 3 x 3 x 3 with 4 nonzeros
	(1,1,1)     1
	(2,1,1)     1
	(2,1,3)     1
	(3,2,2)     1
</pre><h2 id="17">Subscripted reference for sumtensors</h2><p>Subscripted reference can be used to return the individual parts of a sumtensor.</p><pre class="codeinput">T.part{1}
T.part{2}
</pre><pre class="codeoutput">ans is a tensor of size 3 x 3 x 3
	ans(:,:,1) = 
	     1     1     1
	     1     1     1
	     1     1     1
	ans(:,:,2) = 
	     1     1     1
	     1     1     1
	     1     1     1
	ans(:,:,3) = 
	     1     1     1
	     1     1     1
	     1     1     1
ans is a sparse tensor of size 3 x 3 x 3 with 4 nonzeros
	(1,1,1)     1
	(2,1,1)     1
	(2,2,2)     1
	(3,3,2)     1
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2017a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Sums of structured tensors
% When certain operations are performed on a tensor which is formed as a 
% sum of tensors, it can be beneficial to avoid explicitly forming the sum.
% For example, if a tensor is formed as a sum of a low rank tensor and a 
% sparse tensor, the structure of the summands can make storage, decomposition and
% operations with other tensors significantly more efficient. The tensor
% toolbox supports a |sumtensor| object designed to exploit this structure.
% Here we explain the basics of defining and using sumtensors.
%% Creating sumtensors
% A sumtensor T can only be delared as a sum of same-sized tensors |T1,
% T2,...,TN|. The summand tensors are stored in a cell array, which define
% the "parts" of the sumtensor. The parts of a sumtensor can be (generic) 
% tensors (as |tensor|), sparse tensors (as |sptensor|), Kruskal tensors 
% (as |ktensor|), or Tucker tensors (as |ttensor|). An example of the use
% of the |sumtensor| constructor follows.
T1 = tensor(ones(3,3,3)); %<REPLACE_WITH_DASH_DASHA tensor
T2 = sptensor([1 1 1; 2 2 2; 3 3 2; 2 1 1], 1, [3,3,3]); %<REPLACE_WITH_DASH_DASHA sparse tensor

T = sumtensor(T1,T2)

%% An Large-Scale Example
% For large-scale problems, the |sumtensor| class may make the difference
% as to whether or not a tensor can be stored in memory. Consider the
% following example, where $\mathcal{T}$ is of size $1000 x 1000 x 1000$, 
% formed from the sum of a |ktensor| and an |sptensor|.
X1 = rand(500, 3); %Generating some factor matrices
X2 = rand(500, 3); 
X3 = rand(500, 3);
K = ktensor([1; 1; 1], X1, X2, X3);
S = sptenrand([500, 500, 500], 1e-100);

ST = sumtensor(K,S); %<REPLACE_WITH_DASH_DASH Declare the sumtensor
TT = full(ST); %<REPLACE_WITH_DASH_DASH Form the sum of the tensors explicitly

whos ST TT %<REPLACE_WITH_DASH_DASHOutput the storage information for these variables

%%
% The difference in memory between the full and sumtensor is a factor of 10^5!
% Hence we prefer to use the sumtensor object whenever possible.
%% Further examples of the sumtensor constructer
% The sumtensor constructor can declare an empty sumtensor object, having 
% no parts, as follows
P = sumtensor()
%%
% |sumtensor| also supports use as a copy constructor.
S = sumtensor(P)
%% Use ndims and size for the dimensions of a sumtensor
% For a given sumtensor, |ndims| returns the number of modes of a sumtensor.
% Similarly, |size| returns a size array of the sumtensor.
ndims(T)
size(T)
%% Use full to convert a sumtensor to a "generic" tensor
% The |full| function can be used to convert a sumtensor to a generic tensor. Note that
% for large-scale tensors, this can a large amount of memory because each part of
% the sumtensor will be expanded and then summed.
full(T)
%% Use double to convert a sumtensor to a multidimensional array
% The |double| function can be used to convert a sumtensor to a multidimensional array.
% Similarly to the |full| expansion, this can use a prohibitive amount of
% memory for large-scale problems.
double(T)
%% Matricized Khatri-Rao product of a sumtensor
% The |mttkrp| function computes the Khatri-Rao product of a matricized tensor and a
% sumtensor. The required arguments are: a sumtensor X, a cell array of
% matrices U={U1,...,Um}, and a mode n. The cell array must consist of m matrices, 
% where m is the number of modes in X. The number of columns of these matrices
% should be constant, and number of rows of matrix Ui should match the dimension
% of the tensor X in mode i. The matricized Khatri-Rao product operation on 
% sumtensor distributes the operation to the summands of the sumtensor. 
% For details of this specific computation, see the mttkrp documentation 
% for a generic tensor. An example of the use of |mttkrp| follows.
U={eye(3), ones(3,3), randn(3,3)}; %<REPLACE_WITH_DASH_DASHThe cell array of matrices
mttkrp(T,U,2)
%% Use innerprod to compute the inner product of a sumtensor
% The |innerprod| function computes the inner product of a sumtensor T and any type of 
% tensor. The operation is performed by distributing across each of the
% sumtensor's parts.
S = sptensor([1 1 1; 2 1 3; 3 2 2; 2 1 1], 1, [3,3,3]);
innerprod(T,S)
%% Use norm for compatibility with the other types of tensors.
% The |norm| function returns 0 and a warning when called on a sumtensor. 
% The procedure of computing the Frobenius norm of a sumtensor
% does not distribute across its parts, and hence is not supported for
% sumtensors. This default behavior is provided in order to ensure 
% compatibility of the sumtensor class with existing decomposition routines.
norm(T)
%%
% In order avoid this default behavior and return the Frobenius norm of a 
% sumtensor, it can be converted to a tensor using |full|.
norm(full(T))
%% Use cp_als to find a CP decomposition of a sumtensor
% One of the primary motivations for defining the |sumtensor| class is for
% efficient decomposition. In particular, when trying to find a CP
% decomposition of a tensor using alternating least squares, the
% subproblems can be efficiently created and solved using |mttkrp| and
% |innerprod|. Both of these operations can be performed more efficiently
% by exploiting extra structure in the tensors which form the sum, so the
% performance of |cp_als| is also improved. Consider the following example,
% where a |cp_als| is run on a sumtensor.
cp_als(T, 2)
%%
% It follows that in cases where $\mathcal{T}$ is too large for its full expansion to be 
% stored in memory, we may still be able find a CP decomposition by exploiting the 
% sumtensor structure. 
%%
% Note that the fit returned by cp_als is not correct for sumtensors, 
% because the norm operation is not supported.
%% Basic operations (plus) for sumtensors
% Sumtensors can be added to any other type of tensor. The result is a new
% sumtensor with the tensor appended to the parts of the original
% sumtensor. Note that the tensor is always appended, despite the order of
% the operation. 
T+S %<REPLACE_WITH_DASH_DASHS is appended to the parts of T
S+T %<REPLACE_WITH_DASH_DASHS is still the last part of T, despite order
%% Subscripted reference for sumtensors
% Subscripted reference can be used to return the individual parts of a 
% sumtensor.
T.part{1}
T.part{2}

##### SOURCE END #####
--></body></html>